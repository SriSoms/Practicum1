{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project 02 - Brian Bell"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment Objective"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset used in this project was a colleciton of approximately 231,000 video game reviews taken from Amazon. The objective of this experiment is to use the contents of a users review to be able to accurately predict the rating. The ratings range from 1-5. To improve model performance, the ratings were changed from 1-3 (1-2=Unfavorable, 3=Neutral, 4-5=Favorable).\n",
    "\n",
    "The dataset was created by Julian McAuley at UCSD. McAuley made the dataset availble to anyone who wishes to use it, as long as the following citations are present:\n",
    "\n",
    "Ups and downs: Modeling the visual evolution of fashion trends with one-class collaborative filtering\n",
    "R. He, J. McAuley\n",
    "WWW, 2016\n",
    "http://cseweb.ucsd.edu/~jmcauley/pdfs/www16a.pdf\n",
    "\n",
    "Image-based recommendations on styles and substitutes\n",
    "J. McAuley, C. Targett, J. Shi, A. van den Hengel\n",
    "SIGIR, 2015\n",
    "http://cseweb.ucsd.edu/~jmcauley/pdfs/sigir15.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import gzip\n",
    "import requests\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = r'http://snap.stanford.edu/data/amazon/productGraph/categoryFiles/reviews_Video_Games_5.json.gz'\n",
    "file = 'reviews_Video_Games_5.json.gz'\n",
    "\n",
    "if not os.path.isfile(file):\n",
    "    r = requests.get(url)\n",
    "    open('reviews_Video_Games_5.json.gz', 'wb').write(r.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code to convert .gz to dataframe from the creator of the dataset at http://jmcauley.ucsd.edu/data/amazon/\n",
    "def parse(path):\n",
    "    g = gzip.open(path, 'rb')\n",
    "    for l in g:\n",
    "        yield eval(l)\n",
    "\n",
    "def getDF(path):\n",
    "    i = 0\n",
    "    df = {}\n",
    "    for d in parse(path):\n",
    "        df[i] = d\n",
    "        i += 1\n",
    "    return pd.DataFrame.from_dict(df, orient='index')\n",
    "\n",
    "df = getDF('reviews_Video_Games_5.json.gz')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>reviewerID</th>\n",
       "      <th>asin</th>\n",
       "      <th>reviewerName</th>\n",
       "      <th>helpful</th>\n",
       "      <th>reviewText</th>\n",
       "      <th>overall</th>\n",
       "      <th>summary</th>\n",
       "      <th>unixReviewTime</th>\n",
       "      <th>reviewTime</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>A2HD75EMZR8QLN</td>\n",
       "      <td>0700099867</td>\n",
       "      <td>123</td>\n",
       "      <td>[8, 12]</td>\n",
       "      <td>Installing the game was a struggle (because of...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Pay to unlock content? I don't think so.</td>\n",
       "      <td>1341792000</td>\n",
       "      <td>07 9, 2012</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A3UR8NLLY1ZHCX</td>\n",
       "      <td>0700099867</td>\n",
       "      <td>Alejandro Henao \"Electronic Junky\"</td>\n",
       "      <td>[0, 0]</td>\n",
       "      <td>If you like rally cars get this game you will ...</td>\n",
       "      <td>4.0</td>\n",
       "      <td>Good rally game</td>\n",
       "      <td>1372550400</td>\n",
       "      <td>06 30, 2013</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>A1INA0F5CWW3J4</td>\n",
       "      <td>0700099867</td>\n",
       "      <td>Amazon Shopper \"Mr.Repsol\"</td>\n",
       "      <td>[0, 0]</td>\n",
       "      <td>1st shipment received a book instead of the ga...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Wrong key</td>\n",
       "      <td>1403913600</td>\n",
       "      <td>06 28, 2014</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>A1DLMTOTHQ4AST</td>\n",
       "      <td>0700099867</td>\n",
       "      <td>ampgreen</td>\n",
       "      <td>[7, 10]</td>\n",
       "      <td>I got this version instead of the PS3 version,...</td>\n",
       "      <td>3.0</td>\n",
       "      <td>awesome game, if it did not crash frequently !!</td>\n",
       "      <td>1315958400</td>\n",
       "      <td>09 14, 2011</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>A361M14PU2GUEG</td>\n",
       "      <td>0700099867</td>\n",
       "      <td>Angry Ryan \"Ryan A. Forrest\"</td>\n",
       "      <td>[2, 2]</td>\n",
       "      <td>I had Dirt 2 on Xbox 360 and it was an okay ga...</td>\n",
       "      <td>4.0</td>\n",
       "      <td>DIRT 3</td>\n",
       "      <td>1308009600</td>\n",
       "      <td>06 14, 2011</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       reviewerID        asin                        reviewerName  helpful  \\\n",
       "0  A2HD75EMZR8QLN  0700099867                                 123  [8, 12]   \n",
       "1  A3UR8NLLY1ZHCX  0700099867  Alejandro Henao \"Electronic Junky\"   [0, 0]   \n",
       "2  A1INA0F5CWW3J4  0700099867          Amazon Shopper \"Mr.Repsol\"   [0, 0]   \n",
       "3  A1DLMTOTHQ4AST  0700099867                            ampgreen  [7, 10]   \n",
       "4  A361M14PU2GUEG  0700099867        Angry Ryan \"Ryan A. Forrest\"   [2, 2]   \n",
       "\n",
       "                                          reviewText  overall  \\\n",
       "0  Installing the game was a struggle (because of...      1.0   \n",
       "1  If you like rally cars get this game you will ...      4.0   \n",
       "2  1st shipment received a book instead of the ga...      1.0   \n",
       "3  I got this version instead of the PS3 version,...      3.0   \n",
       "4  I had Dirt 2 on Xbox 360 and it was an okay ga...      4.0   \n",
       "\n",
       "                                           summary  unixReviewTime  \\\n",
       "0         Pay to unlock content? I don't think so.      1341792000   \n",
       "1                                  Good rally game      1372550400   \n",
       "2                                        Wrong key      1403913600   \n",
       "3  awesome game, if it did not crash frequently !!      1315958400   \n",
       "4                                           DIRT 3      1308009600   \n",
       "\n",
       "    reviewTime  \n",
       "0   07 9, 2012  \n",
       "1  06 30, 2013  \n",
       "2  06 28, 2014  \n",
       "3  09 14, 2011  \n",
       "4  06 14, 2011  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For testing on smaller dataframe\n",
    "# df = df[:500]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.overall = df.overall.map({1.0:1, 2.0:1, 3.0:2, 4.0:3, 5.0:3})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = (df.reviewText.values, df.overall.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/jovyan/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /home/jovyan/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from sklearn.base import BaseEstimator, TransformerMixin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextTransformer(BaseEstimator, TransformerMixin):\n",
    "    '''\n",
    "    This transformer removes numbers, stop words, punctuation, and stemming (with the provided stemmer).\n",
    "    \n",
    "    Expects a column in the dataframe named \"reviewText\" and puts the cleaned text in a column \"reviewTextClean\". \n",
    "    '''\n",
    "    def __init__(self, stemmer, remove_stop=True):\n",
    "        '''\n",
    "        Args: \n",
    "            stemmer: A stemmer with a \"stem\" method\n",
    "            remove_stop: True if stop words should be removed\n",
    "        '''\n",
    "        self.stemmer = stemmer\n",
    "        self.remove_stop=remove_stop      \n",
    "\n",
    "        \n",
    "    def transform(self, X, **transform_params):\n",
    "        '''\n",
    "        Cleans the text\n",
    "        '''\n",
    "        return [self.stem_text(x) for x in X]\n",
    "    \n",
    "    def fit(self, X, y=None, **fit_params):\n",
    "        '''\n",
    "        Fits the transformer\n",
    "        '''\n",
    "        return self\n",
    "    \n",
    "    def stem_text(self, text):\n",
    "        '''\n",
    "        Utility function for transform method to clean the text\n",
    "        '''\n",
    "        text = re.sub(r'http://.*', '', text) # Remove any links\n",
    "        \n",
    "        if self.remove_stop:\n",
    "            stop = self.stop = stopwords.words('english')             \n",
    "        else:\n",
    "            stop = []\n",
    "        \n",
    "        \n",
    "        return ' '.join([self.stemmer.stem(word) for word in word_tokenize(text) if word.isalpha() and word not in stop])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Optimization and Serialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.stem.lancaster import LancasterStemmer\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.linear_model import SGDClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Find Optimal Hyperparameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Originally, transforming the text was part of the pipeline. However, on such a large dataset this is a very expensive task. It takes close to two minutes to transform the text on this computer. Moving it outside of the pipeline, and fitting a separate pipeline for each stemmer, dramatically reduced the run time since it only has to be ran three times instead of hundreds of times. It also also the grid search to run on all available CPU cores, since the tokenize function cannot be ran on all cores in Windows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "porter = PorterStemmer()\n",
    "lancaster = LancasterStemmer()\n",
    "snowball = SnowballStemmer(\"english\")\n",
    "\n",
    "X_porter = TextTransformer(porter).transform(X_train)\n",
    "X_lancaster = TextTransformer(lancaster).transform(X_train)\n",
    "X_snowball = TextTransformer(snowball).transform(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf = TfidfVectorizer()\n",
    "\n",
    "param_grid = [{'vect__max_df' : [.5, .8],\n",
    "               'vect__min_df' : [100, 1000, 10000],\n",
    "               'clf__alpha' : [.0001, .001, .01, .1, 1],\n",
    "               'clf__penalty' : ['l2', 'l1', 'elasticnet'],\n",
    "               'clf__loss' : ['hinge', 'modified_huber', 'log']}]\n",
    "\n",
    "pipe = Pipeline([('vect', tfidf),\n",
    "                ('clf', SGDClassifier(random_state=0))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 270 candidates, totalling 1350 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  18 tasks      | elapsed:   50.2s\n",
      "[Parallel(n_jobs=-1)]: Done 168 tasks      | elapsed:  4.8min\n",
      "[Parallel(n_jobs=-1)]: Done 418 tasks      | elapsed: 11.0min\n",
      "[Parallel(n_jobs=-1)]: Done 768 tasks      | elapsed: 19.8min\n",
      "[Parallel(n_jobs=-1)]: Done 1218 tasks      | elapsed: 32.5min\n",
      "[Parallel(n_jobs=-1)]: Done 1350 out of 1350 | elapsed: 36.0min finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=5,\n",
       "             estimator=Pipeline(steps=[('vect', TfidfVectorizer()),\n",
       "                                       ('clf', SGDClassifier(random_state=0))]),\n",
       "             n_jobs=-1,\n",
       "             param_grid=[{'clf__alpha': [0.0001, 0.001, 0.01, 0.1, 1],\n",
       "                          'clf__loss': ['hinge', 'modified_huber', 'log'],\n",
       "                          'clf__penalty': ['l2', 'l1', 'elasticnet'],\n",
       "                          'vect__max_df': [0.5, 0.8],\n",
       "                          'vect__min_df': [100, 1000, 10000]}],\n",
       "             scoring='accuracy', verbose=1)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gs_porter = GridSearchCV(pipe, param_grid,\n",
    "                  scoring='accuracy',\n",
    "                  cv=5, verbose=1,\n",
    "                  n_jobs=-1)\n",
    "gs_porter.fit(X_porter, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'clf__alpha': 0.0001, 'clf__loss': 'modified_huber', 'clf__penalty': 'l2', 'vect__max_df': 0.8, 'vect__min_df': 100}\n"
     ]
    }
   ],
   "source": [
    "print(gs_porter.best_params_)\n",
    "clf_porter = gs_porter.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 270 candidates, totalling 1350 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  18 tasks      | elapsed:  2.7min\n",
      "[Parallel(n_jobs=-1)]: Done 168 tasks      | elapsed:  6.6min\n",
      "[Parallel(n_jobs=-1)]: Done 418 tasks      | elapsed: 12.7min\n",
      "[Parallel(n_jobs=-1)]: Done 768 tasks      | elapsed: 21.3min\n",
      "[Parallel(n_jobs=-1)]: Done 1218 tasks      | elapsed: 33.7min\n",
      "[Parallel(n_jobs=-1)]: Done 1350 out of 1350 | elapsed: 37.0min finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=5,\n",
       "             estimator=Pipeline(steps=[('vect', TfidfVectorizer()),\n",
       "                                       ('clf', SGDClassifier(random_state=0))]),\n",
       "             n_jobs=-1,\n",
       "             param_grid=[{'clf__alpha': [0.0001, 0.001, 0.01, 0.1, 1],\n",
       "                          'clf__loss': ['hinge', 'modified_huber', 'log'],\n",
       "                          'clf__penalty': ['l2', 'l1', 'elasticnet'],\n",
       "                          'vect__max_df': [0.5, 0.8],\n",
       "                          'vect__min_df': [100, 1000, 10000]}],\n",
       "             scoring='accuracy', verbose=1)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gs_lancaster = GridSearchCV(pipe, param_grid,\n",
    "                  scoring='accuracy',\n",
    "                  cv=5, verbose=1,\n",
    "                  n_jobs=-1)\n",
    "gs_lancaster.fit(X_lancaster, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'clf__alpha': 0.0001, 'clf__loss': 'modified_huber', 'clf__penalty': 'l2', 'vect__max_df': 0.5, 'vect__min_df': 100}\n"
     ]
    }
   ],
   "source": [
    "print(gs_lancaster.best_params_)\n",
    "clf_lancaster = gs_porter.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 270 candidates, totalling 1350 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  18 tasks      | elapsed:   47.1s\n",
      "[Parallel(n_jobs=-1)]: Done 168 tasks      | elapsed:  4.6min\n",
      "[Parallel(n_jobs=-1)]: Done 418 tasks      | elapsed: 10.8min\n",
      "[Parallel(n_jobs=-1)]: Done 768 tasks      | elapsed: 19.5min\n",
      "[Parallel(n_jobs=-1)]: Done 1218 tasks      | elapsed: 31.8min\n",
      "[Parallel(n_jobs=-1)]: Done 1350 out of 1350 | elapsed: 35.3min finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=5,\n",
       "             estimator=Pipeline(steps=[('vect', TfidfVectorizer()),\n",
       "                                       ('clf', SGDClassifier(random_state=0))]),\n",
       "             n_jobs=-1,\n",
       "             param_grid=[{'clf__alpha': [0.0001, 0.001, 0.01, 0.1, 1],\n",
       "                          'clf__loss': ['hinge', 'modified_huber', 'log'],\n",
       "                          'clf__penalty': ['l2', 'l1', 'elasticnet'],\n",
       "                          'vect__max_df': [0.5, 0.8],\n",
       "                          'vect__min_df': [100, 1000, 10000]}],\n",
       "             scoring='accuracy', verbose=1)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gs_snowball = GridSearchCV(pipe, param_grid,\n",
    "                  scoring='accuracy',\n",
    "                  cv=5, verbose=1,\n",
    "                  n_jobs=-1)\n",
    "gs_snowball.fit(X_lancaster, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'clf__alpha': 0.0001, 'clf__loss': 'modified_huber', 'clf__penalty': 'l2', 'vect__max_df': 0.5, 'vect__min_df': 100}\n"
     ]
    }
   ],
   "source": [
    "print(gs_snowball.best_params_)\n",
    "clf_snowball = gs_snowball.best_estimator_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save all the classifiers, in case I need to restart the Jupyter session."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "pickle.dump(clf_porter, open('classifier_porter.pkl', 'wb'), protocol=4)\n",
    "pickle.dump(clf_lancaster, open('classifier_lancaster.pkl', 'wb'), protocol=4)\n",
    "pickle.dump(clf_snowball, open('classifier_snowball.pkl', 'wb'), protocol=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8245232548105963"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf_porter.score(X_porter, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8051945810682544"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf_lancaster.score(X_lancaster, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8090545632352518"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf_snowball.score(X_snowball, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Porter stemmer had the best accuracy on the training data. Get accuracy on test dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_transform = TextTransformer(porter).transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8172059711795668"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf_porter.score(X_test_transform, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The training accuracy was only slightly better that the test accuracy, so the model there was not significant overfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train Best Model on full dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_transform_full = TextTransformer(porter).transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_pipe = Pipeline([('vect', tfidf),\n",
    "                       ('clf', SGDClassifier(random_state=0))])\n",
    "\n",
    "final_pipe.set_params(**gs_porter.best_params_)\n",
    "final_pipe.fit(X_transform_full, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(final_pipe, open('website/pkl/review_classifier.pkl', 'wb'), protocol=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(stopwords.words('english') , open('website/pkl/stop.pkl', 'wb'), protocol=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(porter, open('website/pkl/porter.pkl', 'wb'), protocol=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(TextTransformer(porter), open('website/pkl/transformer.pkl', 'wb'), protocol=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Website Creation and Publishing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make more portable tokenizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "\n",
    "stop = pickle.load(open('website/pkl/stop.pkl', 'rb'))\n",
    "stemmer = pickle.load(open('website/pkl/porter.pkl', 'rb'))\n",
    "\n",
    "def word_tokenize(text):\n",
    "    '''\n",
    "    Removes punctuation and splits into a list.\n",
    "    '''\n",
    "    text = text.translate(str.maketrans('', '', string.punctuation))\n",
    "    text = re.sub(r'\\n', '', text)\n",
    "    \n",
    "    return text.split(\" \")\n",
    "    \n",
    "\n",
    "\n",
    "def stem_text(text):\n",
    "    '''\n",
    "    Stem words in a list.\n",
    "    '''\n",
    "    text = re.sub(r'http://.*', '', text) # Remove any links\n",
    "\n",
    "    return ' '.join([stemmer.stem(word) for word in word_tokenize(text) if word.isalpha() and word not in stop])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train HashVectorizer and pickle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import HashingVectorizer\n",
    "\n",
    "vect = HashingVectorizer(decode_error='ignore',\n",
    "                        n_features=2**21,\n",
    "                        preprocessor=None,\n",
    "                        tokenizer=stem_text)\n",
    "\n",
    "clf = SGDClassifier(loss='modified_huber', alpha=.0001, penalty='l2', random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_stream(X, y, batch_size=1000):\n",
    "    length = len(X)\n",
    "    current = 0 \n",
    "    while current < length:\n",
    "        if current + batch_size < length:\n",
    "            yield (X[current:current+batch_size], y[current:current+batch_size])\n",
    "        else:\n",
    "            yield (X[current:], y[current:])\n",
    "        current += batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = np.unique(y)\n",
    "for X_train, y_train in data_stream(X, y):\n",
    "    X_train = vect.transform(X_train)\n",
    "    clf.partial_fit(X_train, y_train, classes=classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7550090603158167"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.score(vect.transform(X), y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(clf, open('website/pkl/hashing_classifier.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Flask Website"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "http://bell0x2a.pythonanywhere.com/"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
